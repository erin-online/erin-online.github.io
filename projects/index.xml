<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Projects on Terminal</title><link>https://erin-online.github.io/projects/</link><description>Recent content in Projects on Terminal</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Tue, 27 Jun 2023 00:00:00 +0000</lastBuildDate><atom:link href="https://erin-online.github.io/projects/index.xml" rel="self" type="application/rss+xml"/><item><title>Funny Wacky Lines</title><link>https://erin-online.github.io/projects/funny_wacky_lines/</link><pubDate>Tue, 27 Jun 2023 00:00:00 +0000</pubDate><guid>https://erin-online.github.io/projects/funny_wacky_lines/</guid><description>&lt;h2 id="project-details"&gt;Project Details&lt;/h2&gt;
&lt;p&gt;When I was looking at the math for how neural networks worked, I got a little skeptical about the math used in them. Since the learning process used nothing but basic calculus principles, in theory there was nothing stopping me from using any function I wanted&amp;ndash; and in practice, it could maybe even be more efficient in some situations.&lt;/p&gt;
&lt;p&gt;The finished code is essentially a general-purpose framework that I could use to put in any math function and get a neural network that uses it. I tested the resulting networks on simple patterns as a proof-of-concept that they can work.&lt;/p&gt;</description><content>&lt;h2 id="project-details"&gt;Project Details&lt;/h2&gt;
&lt;p&gt;When I was looking at the math for how neural networks worked, I got a little skeptical about the math used in them. Since the learning process used nothing but basic calculus principles, in theory there was nothing stopping me from using any function I wanted&amp;ndash; and in practice, it could maybe even be more efficient in some situations.&lt;/p&gt;
&lt;p&gt;The finished code is essentially a general-purpose framework that I could use to put in any math function and get a neural network that uses it. I tested the resulting networks on simple patterns as a proof-of-concept that they can work.&lt;/p&gt;
&lt;p&gt;You can find the repository &lt;a href="https://github.com/erin-online/lisp-neural"&gt;here&lt;/a&gt;. This code is definitely better than last time but still isn&amp;rsquo;t very suited for use by other people. If you want me to make it more accessible, just message me on &lt;a href="https://twitter.com/cityposting"&gt;Twitter&lt;/a&gt; or on Discord at cityposting.&lt;/p&gt;
&lt;h2 id="what-does-it-mean-for-a-neural-network-to-use-a-math-function"&gt;What does it mean for a neural network to use a math function?&lt;/h2&gt;
&lt;p&gt;Basically, neural networks are functions with a lot of variables that try to match a pattern as closely as possible. The specific pattern in question can be anything. How do they do this? Let&amp;rsquo;s look at an example, the ReLU (Rectified Linear Unit) function:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://erin-online.github.io/projects/funny_wacky_lines/img00_relu_function.png" alt="Graph of the ReLU function: y=x if x is greater than 0, and 0 otherwise."&gt;&lt;/p&gt;
&lt;p&gt;So let&amp;rsquo;s say that we have a mysterious function, and we&amp;rsquo;re trying to fit it as best we can. What we can do is first add in two parameters: a simple multiplier (&amp;ldquo;weight&amp;rdquo;) and a simple vertical movement (&amp;ldquo;bias&amp;rdquo;). Then, we can add together several ReLUs, each with their own weight and bias, to produce an arbitrary pattern:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://erin-online.github.io/projects/funny_wacky_lines/img01_weight_and_bias.png" alt="Visualization of weight and bias"&gt; &lt;img src="https://erin-online.github.io/projects/funny_wacky_lines/img09_relus_sum.png" alt="Sum of each of the ReLUs"&gt;&lt;/p&gt;
&lt;p&gt;The more ReLUs we add together (&amp;ldquo;nodes&amp;rdquo;), the more complex a pattern we can make. Additionally, we can feed the resulting pattern into another set of ReLUs (&amp;ldquo;layer&amp;rdquo;), producing a multiplicatively higher rate of complexity:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://erin-online.github.io/projects/funny_wacky_lines/img02_layers.png" alt="Visualization of additional layers"&gt;&lt;/p&gt;
&lt;p&gt;This project basically involves using functions other than ReLU, and adding new parameters to tweak.&lt;/p&gt;
&lt;h2 id="you-know-who-else-likes-recursion-you-know-who-else-likes-recursion"&gt;You know who else likes recursion? You know who else likes recursion?&lt;/h2&gt;
&lt;p&gt;I was looking for a programming language to use for this project because I didn&amp;rsquo;t want to use Python again. I decided to give Common Lisp a try because several people on the fediverse were always talking about it, and it ended up being an incredibly rewarding experience. Truly cannot recommend this language enough.&lt;/p&gt;
&lt;p&gt;The language structure of Common Lisp lends itself particularly well to recursion, which is basically mandatory if you want to program calculus. This is because you can nest functions within each other as many times as you want; for example e^e^e^e^&amp;hellip;^x or sin(sin(&amp;hellip;sin(x)&amp;hellip;)). Recursion basically entails continually processing the same expression until you finally peel away all the layers, and my get-derivative function ended up working like that:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://erin-online.github.io/projects/funny_wacky_lines/img03_getderivative.png" alt="Demonstration of the get-derivative function"&gt;&lt;/p&gt;
&lt;p&gt;The get-derivative function was essentially the backbone of this whole project. Other people have made much better versions of this function, but I was still extremely happy with mine, especially because it&amp;rsquo;s something I&amp;rsquo;ve been wanting to write even before this project specifically.&lt;/p&gt;
&lt;p&gt;I won&amp;rsquo;t go into too much detail about how calculus is used in this project; basically, it&amp;rsquo;s used to automatically tweak the function parameters in order to make them more closely match the pattern. If you&amp;rsquo;re interested in learning more, check out the &lt;a href="https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi"&gt;3blue1brown neural network playlist&lt;/a&gt;, which is what I learned from.&lt;/p&gt;
&lt;h2 id="dual-boot-gaming-or-pain-and-suffering"&gt;Dual Boot Gaming (or: Pain and Suffering)&lt;/h2&gt;
&lt;p&gt;It was really important that this post have some way to visualize the neural networks from the project, because words and numbers alone aren&amp;rsquo;t sufficient to explain them. So I went looking for Common Lisp plotting libraries and found Lisp-Stat, which looked to be well-maintained and did work quite well for me.&lt;/p&gt;
&lt;p&gt;The only problem was that when I tried to load it in future sessions, it just didn&amp;rsquo;t work. It was some foreign library error thing. Since I&amp;rsquo;d already written some code that used the library (not a lot, in retrospect), I made what I thought was the rational choice and tried to fix the error, and was increasingly frustrated by the lack of help from both the compiler and the Internet.&lt;/p&gt;
&lt;p&gt;You might have noticed that it&amp;rsquo;s been over a year since I released my last project. Despite this, I wouldn&amp;rsquo;t say that more than about three to four months of focused work went into this one. Various real-life affairs did get in the way, but I was also discouraged for months by this issue.&lt;/p&gt;
&lt;p&gt;What I didn&amp;rsquo;t want to do was learn how Common Lisp libraries were loaded, which in theory was interesting but in practice was not a trial I wanted to undertake just to finish this project. Eventually, what I did was install OpenBSD on my computer because I was wondering if the issue was with Windows specifically (also, I&amp;rsquo;d been meaning to try a different OS for a while). The library also refused to load there, which convinced me to just give up and use a different library.&lt;/p&gt;
&lt;p&gt;This led me to vgplot, an excellent plotting library with a wonderful built-in demo that made it easy to learn. The hilarious thing about this whole affair is that vgplot worked on OpenBSD for me but not on Windows, because it couldn&amp;rsquo;t find my Windows gnuplot installation (vgplot is built on gnuplot). There&amp;rsquo;s likely an easy fix for this, but I didn&amp;rsquo;t care to solve it because I could just do my programming in OpenBSD.&lt;/p&gt;
&lt;h2 id="funny-wacky-lines"&gt;Funny Wacky Lines&lt;/h2&gt;
&lt;p&gt;I made the following animations by writing a function that repeatedly plotted the network and saved the resulting plot as an image, then putting all the images into ezgif.&lt;/p&gt;
&lt;p&gt;All of these have the same setup: a small network with 2 middle layers of 4 nodes each, trying to approximate the function y=x^2 from x=0 to x=1.&lt;/p&gt;
&lt;h4 id="relu-network"&gt;ReLU Network&lt;/h4&gt;
&lt;p&gt;Activation = relu(w1a1 + w2a2 + w3a3 + &amp;hellip; + wnan + b), where a1, a2, a3 etc are activations from the previous layer, w1, w2, w3 etc are the weight parameters for this node, and b is the bias parameter for this node. This will be hard to understand if you haven&amp;rsquo;t watched a 20-minute explanation on this stuff somewhere.&lt;/p&gt;
&lt;p&gt;Despite being the function of choice for most neural networks nowadays, ReLU was actually inconsistent for me in practice, and would often settle in a suboptimal pattern. I chalked this up mostly to the network being small and thus likely inconsistent.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://erin-online.github.io/projects/funny_wacky_lines/img04_relu_learning.gif" alt="Animation of the ReLU Network"&gt;&lt;/p&gt;
&lt;h4 id="tanh-network"&gt;Tanh Network&lt;/h4&gt;
&lt;p&gt;Activation = tanh(w1a1 + w2a2 + w3a3 + &amp;hellip; + wnan + b).&lt;/p&gt;
&lt;p&gt;tanh is the hyperbolic tangent function, which has also seen some use in standard neural networks, although it is overall less common than ReLU. It looks like this:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://erin-online.github.io/projects/funny_wacky_lines/img05_tanh_plot.png" alt="plot of the tanh function"&gt;&lt;/p&gt;
&lt;p&gt;This one is still pretty standard and there isn&amp;rsquo;t a lot to see here. We aren&amp;rsquo;t getting into the really weird stuff yet.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://erin-online.github.io/projects/funny_wacky_lines/img06_tanh_learning.gif" alt="Animation of the tanh network"&gt;&lt;/p&gt;
&lt;h4 id="sine-network"&gt;Sine Network&lt;/h4&gt;
&lt;p&gt;Activation = wα1*sin(wβ1a1) + wα2*sin(wβ2a2) + wα3*sin(wβ3a3) + &amp;hellip; + wαn*sin(wβnan) + b, where alpha and beta are used to distinguish the weights from each other. This looks more confusing than it is. Note that unlike the ReLU and tanh networks, this one has no overall &amp;ldquo;wrapping&amp;rdquo; function, instead opting to modify each previous activation individually.&lt;/p&gt;
&lt;p&gt;If you&amp;rsquo;re wondering why these plots all go from x=0 to 1, you can thank the sine network for that, because higher x values literally don&amp;rsquo;t work for it (you start getting the same output for low and high x values, in theory it&amp;rsquo;s workable with low wβ values but in practice the gradients don&amp;rsquo;t go out of their way to favor those). Outside of that, though, it actually worked pretty well.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://erin-online.github.io/projects/funny_wacky_lines/img07_sine_learning.gif" alt="Animation of the sine network"&gt;&lt;/p&gt;
&lt;h4 id="exponential-network"&gt;Exponential Network&lt;/h4&gt;
&lt;p&gt;Activation = c * e^(w1a1 + w2a2 + w3a3 + &amp;hellip; + wnan), where c is the coefficient (like the bias except you multiply it instead of adding it)&lt;/p&gt;
&lt;p&gt;This is one of my favorites; I love watching the exponential curves work together to produce a perfect fit. Granted, the x^2 pattern definitely favors this one.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://erin-online.github.io/projects/funny_wacky_lines/img08_exponential_learning.gif" alt="Animation of the exponential network"&gt;&lt;/p&gt;
&lt;p&gt;Originally I wanted to add more weird networks here, but I wasn&amp;rsquo;t really inspired. Maybe I&amp;rsquo;ll just edit this post down the line.&lt;/p&gt;
&lt;h2 id="thoughts-and-takeaways"&gt;Thoughts and Takeaways&lt;/h2&gt;
&lt;p&gt;Lots of stuff to put here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Don&amp;rsquo;t get owned by sunk cost fallacy. Throw away things if it&amp;rsquo;ll make the project move along faster.&lt;/li&gt;
&lt;li&gt;Neural networks need to be very fast to do cool stuff. My code was not at all speed-optimized, so I wasn&amp;rsquo;t able to train it on more complex things like the MNIST handwriting database, and even the above animations took a couple minutes each to generate.&lt;/li&gt;
&lt;li&gt;There&amp;rsquo;s probably no need to deviate from the standard ReLU functions for practical purposes. I would maybe be able to give a better answer if I had more of a math background, but I don&amp;rsquo;t.&lt;/li&gt;
&lt;li&gt;For that matter, learning the relevant math (or other subjects) for a project is pretty important. When all you know is how to program stuff, it&amp;rsquo;s easy to get caught up in a vortex without knowing where you are or what you&amp;rsquo;re doing. More theoretical knowledge goes a long way here.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="plans"&gt;Plans&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Smaller-scale projects, more writing, more reading.&lt;/li&gt;
&lt;li&gt;Common Lisp is really cool. In the future I&amp;rsquo;ll probably use existing CL machine learning libraries like clml, which (presumably) are faster.&lt;/li&gt;
&lt;li&gt;I&amp;rsquo;ll probably move this site to a different domain within a year or two. The old domain will redirect to the new one.&lt;/li&gt;
&lt;li&gt;I might learn HTML/CSS at some point to make this website prettier; I&amp;rsquo;ve seen some great-looking indie sites out there recently. This would probably go along with the previous step.&lt;/li&gt;
&lt;li&gt;I happened upon &lt;a href="https://arxiv.org/pdf/2304.06035.pdf"&gt;this article&lt;/a&gt; recently which is pretty relevant to the stuff I&amp;rsquo;ve been doing, so I might draw some inspiration from it for future projects.&lt;/li&gt;
&lt;li&gt;The next projects will probably involve ideas from &lt;a href="https://erin-online.github.io/writings/atb_and_the_minecraft_problem/"&gt;A Thousand Brains and the Minecraft Problem&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;</content></item><item><title>Spiral</title><link>https://erin-online.github.io/projects/spiral/</link><pubDate>Thu, 19 May 2022 00:00:00 +0000</pubDate><guid>https://erin-online.github.io/projects/spiral/</guid><description>&lt;h2 id="project-details"&gt;Project Details&lt;/h2&gt;
&lt;p&gt;In this project, I created an algorithm to play weighted &lt;a href="https://en.wikipedia.org/wiki/Rock_paper_scissors"&gt;rock-paper-scissors&lt;/a&gt;. &amp;ldquo;Weighted&amp;rdquo; here means that depending on which choice you win with, you get different rewards. For example with weights [3, 2, 1], you get 3 points for winning with rock, 2 points for winning with paper, and 1 point for winning with scissors.&lt;/p&gt;
&lt;p&gt;I wanted the algorithm to converge toward &lt;strong&gt;optimal&lt;/strong&gt; (least exploitable) play. In unweighted rock-paper-scissors, this looks like a 1/3 chance to play each choice. When you add in weights, it looks like the following:&lt;/p&gt;</description><content>&lt;h2 id="project-details"&gt;Project Details&lt;/h2&gt;
&lt;p&gt;In this project, I created an algorithm to play weighted &lt;a href="https://en.wikipedia.org/wiki/Rock_paper_scissors"&gt;rock-paper-scissors&lt;/a&gt;. &amp;ldquo;Weighted&amp;rdquo; here means that depending on which choice you win with, you get different rewards. For example with weights [3, 2, 1], you get 3 points for winning with rock, 2 points for winning with paper, and 1 point for winning with scissors.&lt;/p&gt;
&lt;p&gt;I wanted the algorithm to converge toward &lt;strong&gt;optimal&lt;/strong&gt; (least exploitable) play. In unweighted rock-paper-scissors, this looks like a 1/3 chance to play each choice. When you add in weights, it looks like the following:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://erin-online.github.io/projects/spiral/img0_weighted_rps_explanation.png" alt="image"&gt;&lt;/p&gt;
&lt;p&gt;The easiest way to do this is to have the algorithm play itself 3 times to determine the weights, then set itself to the optimal strategy instantly. I thought this was a little cheap and wouldn&amp;rsquo;t make me learn much, so I looked at what else I could come up with.&lt;/p&gt;
&lt;p&gt;I didn&amp;rsquo;t want to use a neural network for this because it looked like an overcomplicated solution to a simple problem. I wanted to work with something I could understand.&lt;/p&gt;
&lt;h2 id="initial-algorithm"&gt;Initial Algorithm&lt;/h2&gt;
&lt;p&gt;The algorithm I worked with was extremely simple, and worked similarly to the &lt;a href="https://en.wikipedia.org/wiki/Matchbox_Educable_Noughts_and_Crosses_Engine"&gt;MENACE&lt;/a&gt;. It stored three values, one for each choice, and increased or decreased them whenever it played against itself.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://erin-online.github.io/projects/spiral/img1_initial_algo_explanation.png" alt="image"&gt;&lt;/p&gt;
&lt;p&gt;The bulk of my tests consisted of starting all the algorithm&amp;rsquo;s values as equal, such as (100, 100, 100), then giving it an asymmetrical weight set such as [2, 1, 1] and seeing if it could find its way to the optimal strategy.&lt;/p&gt;
&lt;p&gt;I expected this to be fairly straightforward, but ran into confounding results. Rather than converge, each value continued to oscillate similar to a sine wave. Additionally, over time the oscillations became broader.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://erin-online.github.io/projects/spiral/img2_initial_behavior.png" alt="image"&gt;&lt;/p&gt;
&lt;p&gt;This eventually led to &lt;em&gt;extinction&lt;/em&gt;, which is where one option hits 0 and so is never played again, leaving the option it beats to dominate.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://erin-online.github.io/projects/spiral/img3_extinction_example.png" alt="image"&gt;&lt;/p&gt;
&lt;h2 id="smooth-rps"&gt;Smooth RPS&lt;/h2&gt;
&lt;p&gt;In order to figure out what was going on here, I decided to analyze the sine-like function that the algorithm was producing and see if I could figure out its equation. This was a terrible idea, but it sounded good at the time.&lt;/p&gt;
&lt;p&gt;Anyway, the first thing I needed to do was get rid of the randomness involved in option selection, so I came up with Smooth RPS. Basically, rather than simulating an actual game of rock-paper-scissors, Smooth RPS predicts how much the algorithm will change, on average, and makes that change. Here&amp;rsquo;s an example:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://erin-online.github.io/projects/spiral/img4_smooth_rps.png" alt="image"&gt;&lt;/p&gt;
&lt;p&gt;This also helpfully eliminated extinction, because the lower a choice&amp;rsquo;s value got, the less likely it was to be played, meaning the value didn&amp;rsquo;t decrease by as much.&lt;/p&gt;
&lt;h2 id="mathematical-function-hell"&gt;Mathematical Function Hell&lt;/h2&gt;
&lt;p&gt;Here was the function Smooth RPS gave me for each choice:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://erin-online.github.io/projects/spiral/img5_f_demonstration.png" alt="image"&gt;&lt;/p&gt;
&lt;p&gt;This is a little bit different from a sine wave:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://erin-online.github.io/projects/spiral/img6_f_vs_sine.png" alt="image"&gt;&lt;/p&gt;
&lt;p&gt;By scouring Wikipedia, I found the &lt;a href="https://en.wikipedia.org/wiki/Clausen_function"&gt;Clausen function&lt;/a&gt;, which is a really cool function involving summation of sines. There are a lot of ways to mess with it, and I spent several hours on it but was ultimately unable to have it line up with the line my algorithm produced.&lt;/p&gt;
&lt;p&gt;At this point, I was feeling pretty discouraged. My adventure through weird math stuff was fun, but I had no idea how to actually make progress in the project. But it wasn&amp;rsquo;t over yet.&lt;/p&gt;
&lt;h2 id="spiral"&gt;Spiral&lt;/h2&gt;
&lt;p&gt;My next idea was representing the data on a &lt;a href="https://en.wikipedia.org/wiki/Barycentric_coordinate_system"&gt;barycentric graph&lt;/a&gt;. Up until this point, I&amp;rsquo;d been convinced that I couldn&amp;rsquo;t model all three variables at once without using a 3D graph. Putting them on a triangle changed that.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://erin-online.github.io/projects/spiral/img7_barycentric.png" alt="image"&gt;&lt;/p&gt;
&lt;p&gt;When I modeled the adaptation of the algorithm on it, I was face-to-face with the spiral for the first time:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://erin-online.github.io/projects/spiral/img8_spiral_out.png" alt="image"&gt;&lt;/p&gt;
&lt;p&gt;(Note: The white square in the middle is the optimal strategy.)&lt;/p&gt;
&lt;p&gt;Suddenly everything became clear.&lt;/p&gt;
&lt;p&gt;The algorithm trended towards a circular pattern because it was trying to win against itself more. Optimal strategies are of little use when they can never give you any advantage. If your friend is always playing rock against you, then you should start playing paper more frequently, even though that loses to someone who plays scissors.&lt;/p&gt;
&lt;p&gt;Why, then, did it spiral out of control? This was due to Smooth RPS not being smooth enough. By changing the algorithm a bit at a time through iterations, it essentially tried to simulate a circle using straight lines, which resulted in it growing larger over time:&lt;/p&gt;
&lt;p&gt;&lt;img src="https://erin-online.github.io/projects/spiral/img9_tangent.png" alt="image"&gt;&lt;/p&gt;
&lt;p&gt;The solution was also clear. The spiral was constantly moving &lt;em&gt;away&lt;/em&gt; from the optimal strategy, but I could simply re-engineer it to turn it &lt;em&gt;inward&lt;/em&gt; instead.&lt;/p&gt;
&lt;p&gt;What I did for this was take every algorithm change, convert it into barycentric coordinates, rotate it counterclockwise by 90 degrees, then convert it back into value changes.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://erin-online.github.io/projects/spiral/img10_spiral_algo.png" alt="image"&gt;&lt;/p&gt;
&lt;p&gt;This worked exceedingly well. You can see the algorithm instantly seeking out the optimal strategy.&lt;/p&gt;
&lt;p&gt;&lt;img src="https://erin-online.github.io/projects/spiral/img11_spiral.png" alt="image"&gt;&lt;/p&gt;
&lt;h2 id="thoughts-and-takeaways"&gt;Thoughts and Takeaways&lt;/h2&gt;
&lt;p&gt;I&amp;rsquo;m really happy with how this project turned out. The solution was very elegant and gave me some neat things to take away.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The concept of &amp;ldquo;breaking down&amp;rdquo; games is interesting. Fighting games for example are often considered as rock-paper-scissors + frame data and stuff, so if you have all the frame data figured out, then you can simplify the game down to rock-paper-scissors which we have here.&lt;/li&gt;
&lt;li&gt;Ways to visualize many variables at once can be really useful, although barycentric only works for 3 variables in 2D space.&lt;/li&gt;
&lt;li&gt;Machine learning algorithms should have some model of &lt;em&gt;themselves&lt;/em&gt;. This was implemented in the solution here when the coordinates are converted.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id="plans"&gt;Plans&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;I&amp;rsquo;ll probably keep hosting this site on GitHub for now. I plan to mostly directly link it through various online avenues and don&amp;rsquo;t want to go through the time and money of setting up a domain right now.&lt;/li&gt;
&lt;li&gt;Obviously I&amp;rsquo;ll work on a more complicated game than rock-paper-scissors next time. I was thinking of doing something with chess, because it fits a few criteria quite nicely. It&amp;rsquo;s simple and easy to simulate, it has a lot of depth, and I have a personal interest in making sure the current best chess engine, Stockfish, perishes in spectacular fashion for telling me all my awesome and funny moves are &amp;ldquo;inaccurate&amp;rdquo; and &amp;ldquo;blunders&amp;rdquo;.&lt;/li&gt;
&lt;li&gt;I don&amp;rsquo;t want to use Python anymore. I know it&amp;rsquo;s often used for machine learning, but it just does not work for me. My code by the end was an absolute mess and debugging took a couple hours. I know this is partly me just being a bad programmer, but when I&amp;rsquo;ve used languages like Java in the past I feel like they do a better job forcing me to get some groundwork in place. I&amp;rsquo;ll go language shopping which will probably slow me down a bit, but this is fine, I&amp;rsquo;m in no rush.&lt;/li&gt;
&lt;li&gt;On that note, I&amp;rsquo;ve been meaning to try operating systems besides Windows. I&amp;rsquo;ve been on Windows machines for my whole life simply because it&amp;rsquo;s the status quo and there&amp;rsquo;s nothing new for me to wrap my head around. I think it&amp;rsquo;s time for me to actually make an effort to improve my experience using my computer. (Recommendations are welcome.)&lt;/li&gt;
&lt;/ul&gt;</content></item></channel></rss>